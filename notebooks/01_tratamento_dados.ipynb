{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6e58a43",
   "metadata": {},
   "source": [
    "## Importacao de Bibliotecas\n",
    "\n",
    "Carrega todas as bibliotecas necessarias para o pipeline de tratamento. O modulo `neon_utils` fornece a classe `NeonConnection` que encapsula a logica de conexao com o banco Neon PostgreSQL. As bibliotecas `pandas` e `numpy` sao utilizadas para manipulacao e transformacao dos dados. O modulo `pickle` permite a serializacao dos metadados do tratamento para uso posterior no notebook de imputacao."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48e7dfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sys.path.insert(0, str(Path('..') / 'fastapi'))\n",
    "from services.neon_utils import NeonConnection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2858f4",
   "metadata": {},
   "source": [
    "## Conexao com o Banco de Dados Neon\n",
    "\n",
    "Estabelece a conexao com o banco PostgreSQL hospedado no Neon. A classe `NeonConnection` carrega automaticamente as credenciais do arquivo `.env` localizado na raiz do projeto. O metodo `test_connection()` valida se a conexao foi estabelecida corretamente antes de prosseguir com as operacoes de leitura. A propriedade `engine` retorna um objeto SQLAlchemy Engine que sera utilizado para executar queries SQL via pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fed5d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conexao estabelecida com sucesso\n",
      "Database: neondb\n",
      "Host: ep-still-rain-ahoka4v9.c-3.us-east-1.aws.neon.tech\n"
     ]
    }
   ],
   "source": [
    "env_path = Path('..') / '.env'\n",
    "conn = NeonConnection(str(env_path))\n",
    "\n",
    "if not conn.test_connection():\n",
    "    raise ConnectionError(\"Falha na conexao com o banco Neon\")\n",
    "\n",
    "print(f\"Conexao estabelecida com sucesso\")\n",
    "print(f\"Database: {conn.config['database']}\")\n",
    "print(f\"Host: {conn.config['host']}\")\n",
    "\n",
    "engine = conn.engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab57b9ca",
   "metadata": {},
   "source": [
    "## Carregamento dos Dados da Tabela\n",
    "\n",
    "Executa uma query SQL para carregar todos os registros da tabela `dados_meteorologicos`. A query seleciona apenas as colunas relevantes para o processo de tratamento: identificador unico (`id`), nome da estacao meteorologica (`estacao`), data e hora da medicao, e as tres variaveis meteorologicas de interesse (`temperatura`, `umidade`, `velocidade_vento`). Os dados sao ordenados por estacao e data/hora para garantir a sequencia temporal correta de cada serie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59a4f718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de registros carregados: 526,176\n",
      "Shape do DataFrame: (526176, 7)\n",
      "Quantidade de estacoes: 12\n",
      "\n",
      "Estacoes disponiveis:\n",
      "estacao\n",
      "ARCO VERDE       43848\n",
      "CABROBO          43848\n",
      "CARUARU          43848\n",
      "FLORESTA         43848\n",
      "GARANHUNS        43848\n",
      "IBIMIRIM         43848\n",
      "OURICURI         43848\n",
      "PALMARES         43848\n",
      "PETROLINA        43848\n",
      "SALGUEIRO        43848\n",
      "SERRA TALHADA    43848\n",
      "SURUBIM          43848\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "    id,\n",
    "    estacao,\n",
    "    data,\n",
    "    hora,\n",
    "    temperatura,\n",
    "    umidade,\n",
    "    velocidade_vento\n",
    "FROM dados_meteorologicos\n",
    "ORDER BY estacao, data, hora\n",
    "\"\"\"\n",
    "\n",
    "df_raw = pd.read_sql(query, engine)\n",
    "\n",
    "print(f\"Total de registros carregados: {len(df_raw):,}\")\n",
    "print(f\"Shape do DataFrame: {df_raw.shape}\")\n",
    "print(f\"Quantidade de estacoes: {df_raw['estacao'].nunique()}\")\n",
    "print(f\"\\nEstacoes disponiveis:\")\n",
    "print(df_raw['estacao'].value_counts().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b147b9",
   "metadata": {},
   "source": [
    "## Tratamento de Timestamps\n",
    "\n",
    "Converte as colunas separadas de data e hora em um unico campo timestamp. A coluna `hora` no banco esta armazenada no formato string \"HHMM UTC\" (por exemplo, \"0000 UTC\" para meia-noite). O processamento extrai os dois primeiros caracteres para obter a hora como inteiro (0-23), e em seguida combina com a coluna `data` para criar um timestamp completo no formato datetime do pandas. Este timestamp unificado e essencial para a criacao das features temporais e para manter a ordenacao cronologica dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52506fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Periodo dos dados:\n",
      "  Inicio: 2020-01-01 00:00:00\n",
      "  Fim: 2024-12-31 23:00:00\n",
      "  Duracao: 1826 dias\n"
     ]
    }
   ],
   "source": [
    "df_raw['hora_int'] = df_raw['hora'].str[:2].astype(int)\n",
    "\n",
    "df_raw['timestamp'] = pd.to_datetime(\n",
    "    df_raw['data'].astype(str) + ' ' + \n",
    "    df_raw['hora_int'].astype(str).str.zfill(2) + ':00:00'\n",
    ")\n",
    "\n",
    "print(f\"Periodo dos dados:\")\n",
    "print(f\"  Inicio: {df_raw['timestamp'].min()}\")\n",
    "print(f\"  Fim: {df_raw['timestamp'].max()}\")\n",
    "print(f\"  Duracao: {(df_raw['timestamp'].max() - df_raw['timestamp'].min()).days} dias\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb766296",
   "metadata": {},
   "source": [
    "## Analise de Valores Ausentes\n",
    "\n",
    "Quantifica a presenca de valores nulos nas tres variaveis meteorologicas de interesse. Esta analise e fundamental para entender a magnitude do problema de dados faltantes que sera tratado no notebook de imputacao. O percentual de valores ausentes em cada coluna determina a viabilidade e a estrategia de imputacao a ser adotada. Valores faltantes podem ocorrer devido a falhas nos sensores, problemas de transmissao ou periodos de manutencao das estacoes meteorologicas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "801a2559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analise de Valores Ausentes:\n",
      "          Coluna  Valores Faltantes  Percentual (%)\n",
      "     temperatura             147124           27.96\n",
      "         umidade             183691           34.91\n",
      "velocidade_vento             148572           28.24\n"
     ]
    }
   ],
   "source": [
    "target_columns = ['temperatura', 'umidade', 'velocidade_vento']\n",
    "\n",
    "missing_counts = df_raw[target_columns].isnull().sum()\n",
    "missing_percentages = (missing_counts / len(df_raw) * 100).round(2)\n",
    "\n",
    "missing_analysis = pd.DataFrame({\n",
    "    'Coluna': missing_counts.index,\n",
    "    'Valores Faltantes': missing_counts.values,\n",
    "    'Percentual (%)': missing_percentages.values\n",
    "})\n",
    "\n",
    "print(\"Analise de Valores Ausentes:\")\n",
    "print(missing_analysis.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb512cb",
   "metadata": {},
   "source": [
    "## Engenharia de Features Temporais\n",
    "\n",
    "Cria variaveis derivadas do timestamp que capturam padroes temporais relevantes para o processo de imputacao. As features incluem:\n",
    "\n",
    "- **ano, mes, dia, hora**: Componentes basicos do timestamp para capturar sazonalidades\n",
    "- **dia_ano**: Dia do ano (1-365/366), util para capturar ciclos anuais\n",
    "- **dia_semana**: Dia da semana (0-6), onde 0 representa segunda-feira\n",
    "- **hora_sin, hora_cos**: Codificacao ciclica da hora usando funcoes seno e cosseno, garantindo que a hora 23 seja proxima da hora 0\n",
    "- **mes_sin, mes_cos**: Codificacao ciclica do mes para capturar sazonalidade anual de forma continua\n",
    "\n",
    "A codificacao ciclica e preferivel a codificacao linear para variaveis periodicas, pois preserva a relacao de proximidade entre valores nas extremidades do ciclo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31d31fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features temporais criadas: 10\n",
      "Shape apos feature engineering: (526176, 18)\n"
     ]
    }
   ],
   "source": [
    "df = df_raw.sort_values(['estacao', 'timestamp']).copy()\n",
    "\n",
    "df['ano'] = df['timestamp'].dt.year\n",
    "df['mes'] = df['timestamp'].dt.month\n",
    "df['dia'] = df['timestamp'].dt.day\n",
    "df['hora'] = df['hora_int']\n",
    "df['dia_ano'] = df['timestamp'].dt.dayofyear\n",
    "df['dia_semana'] = df['timestamp'].dt.dayofweek\n",
    "\n",
    "df['hora_sin'] = np.sin(2 * np.pi * df['hora'] / 24)\n",
    "df['hora_cos'] = np.cos(2 * np.pi * df['hora'] / 24)\n",
    "df['mes_sin'] = np.sin(2 * np.pi * df['mes'] / 12)\n",
    "df['mes_cos'] = np.cos(2 * np.pi * df['mes'] / 12)\n",
    "\n",
    "print(f\"Features temporais criadas: 10\")\n",
    "print(f\"Shape apos feature engineering: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1e6b6b",
   "metadata": {},
   "source": [
    "## Codificacao One-Hot da Variavel Categorica\n",
    "\n",
    "Aplica codificacao One-Hot Encoding na coluna `estacao`, transformando a variavel categorica em um conjunto de variaveis binarias. Cada estacao meteorologica passa a ser representada por uma coluna propria, contendo valor 1 quando o registro pertence aquela estacao e 0 caso contrario. Esta transformacao e necessaria porque algoritmos de machine learning, incluindo o IterativeImputer, requerem entradas numericas. A codificacao One-Hot preserva a natureza nominal da variavel (nao ha ordem entre as estacoes) e permite que o modelo de imputacao aprenda padroes especificos de cada localidade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc94d295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colunas One-Hot criadas: 12\n",
      "Shape apos encoding: (526176, 29)\n",
      "\n",
      "Colunas de estacao:\n",
      "  - estacao_ARCO VERDE\n",
      "  - estacao_CABROBO\n",
      "  - estacao_CARUARU\n",
      "  - estacao_FLORESTA\n",
      "  - estacao_GARANHUNS\n",
      "  - estacao_IBIMIRIM\n",
      "  - estacao_OURICURI\n",
      "  - estacao_PALMARES\n",
      "  - estacao_PETROLINA\n",
      "  - estacao_SALGUEIRO\n",
      "  - estacao_SERRA TALHADA\n",
      "  - estacao_SURUBIM\n"
     ]
    }
   ],
   "source": [
    "df_encoded = pd.get_dummies(df, columns=['estacao'], prefix='estacao', dtype=int)\n",
    "\n",
    "estacao_columns = [col for col in df_encoded.columns if col.startswith('estacao_')]\n",
    "\n",
    "print(f\"Colunas One-Hot criadas: {len(estacao_columns)}\")\n",
    "print(f\"Shape apos encoding: {df_encoded.shape}\")\n",
    "print(f\"\\nColunas de estacao:\")\n",
    "for col in estacao_columns:\n",
    "    print(f\"  - {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58983322",
   "metadata": {},
   "source": [
    "## Selecao e Organizacao das Colunas Finais\n",
    "\n",
    "Organiza o DataFrame final selecionando apenas as colunas necessarias para o processo de imputacao. A estrutura final inclui:\n",
    "\n",
    "- **Identificadores**: `id` (chave primaria do banco), `data` e `hora` (referencias temporais separadas)\n",
    "- **Colunas alvo**: `temperatura`, `umidade`, `velocidade_vento` - variaveis que contem valores faltantes a serem imputados\n",
    "- **Features**: variaveis temporais (10 colunas) e variaveis de estacao (12 colunas One-Hot)\n",
    "\n",
    "Esta organizacao separa claramente o que sera imputado (colunas alvo) do que sera usado como informacao auxiliar (features), facilitando o processo no notebook de imputacao. As colunas `data` e `hora` sao mantidas separadas conforme estrutura original do banco de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5a85e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset final preparado\n",
      "  Total de registros: 526,176\n",
      "  Colunas identificadoras: id, data, hora (separadas)\n",
      "  Colunas alvo: 3\n",
      "  Features temporais: 10\n",
      "  Features de estacao: 12\n",
      "  Total de colunas: 28\n"
     ]
    }
   ],
   "source": [
    "target_cols = ['temperatura', 'umidade', 'velocidade_vento']\n",
    "\n",
    "temporal_features = [\n",
    "    'ano', 'mes', 'dia', 'hora', 'dia_ano', 'dia_semana',\n",
    "    'hora_sin', 'hora_cos', 'mes_sin', 'mes_cos'\n",
    "]\n",
    "\n",
    "feature_cols = temporal_features + estacao_columns\n",
    "\n",
    "final_columns = ['id', 'data'] + target_cols + feature_cols\n",
    "df_final = df_encoded[final_columns].copy()\n",
    "\n",
    "print(f\"Dataset final preparado\")\n",
    "print(f\"  Total de registros: {len(df_final):,}\")\n",
    "print(f\"  Colunas identificadoras: id, data, hora (separadas)\")\n",
    "print(f\"  Colunas alvo: {len(target_cols)}\")\n",
    "print(f\"  Features temporais: {len(temporal_features)}\")\n",
    "print(f\"  Features de estacao: {len(estacao_columns)}\")\n",
    "print(f\"  Total de colunas: {len(final_columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4698122",
   "metadata": {},
   "source": [
    "## Exportacao do Dataset Tratado por Estacao\n",
    "\n",
    "Serializa o DataFrame tratado e os metadados do pipeline para arquivos pickle separados por estacao. Para cada estacao meteorologica, sao criados:\n",
    "\n",
    "- `dados_tratados_{estacao}.pkl`: Dataset filtrado contendo apenas os registros daquela estacao, com `data` e `hora` preservadas em colunas separadas\n",
    "- `metadata_tratamento_{estacao}.pkl`: Metadados especificos da estacao incluindo nomes das colunas alvo, features, estatisticas e periodo dos dados\n",
    "\n",
    "Esta separacao por estacao facilita o processamento paralelo da imputacao e permite tratamentos customizados para cada localidade. As colunas `data` e `hora` sao mantidas separadas em cada arquivo, preservando a estrutura original do banco de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b540df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exportando dados para 12 estacoes...\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_173/3074293667.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mdf_estacao_temp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_estacao\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     df_estacao_temp['timestamp_temp'] = pd.to_datetime(\n\u001b[1;32m     21\u001b[0m         \u001b[0mdf_estacao_temp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mdf_estacao_temp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hora'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m':00:00'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     )\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     metadata = {\n",
      "\u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   6200\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6201\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6202\u001b[0m         ):\n\u001b[1;32m   6203\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6204\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'str'"
     ]
    }
   ],
   "source": [
    "estacoes_unicas = df_raw['estacao'].unique()\n",
    "\n",
    "print(f\"Exportando dados para {len(estacoes_unicas)} estacoes...\\n\")\n",
    "\n",
    "for estacao in estacoes_unicas:\n",
    "    estacao_col = f'estacao_{estacao}'\n",
    "    if estacao_col not in df_final.columns:\n",
    "        continue\n",
    "    \n",
    "    df_estacao = df_final[df_final[estacao_col] == 1].copy()\n",
    "    df_estacao = df_estacao[['id', 'data'] + target_cols + feature_cols].copy()\n",
    "    \n",
    "    estacao_filename = estacao.replace(' ', '_').replace('/', '_')\n",
    "    pkl_filename = f'dados_tratados_{estacao_filename}.pkl'\n",
    "    metadata_filename = f'metadata_tratamento_{estacao_filename}.pkl'\n",
    "    \n",
    "    df_estacao.to_pickle(pkl_filename)\n",
    "    \n",
    "    df_estacao_temp = df_estacao.copy()\n",
    "    df_estacao_temp['timestamp_temp'] = pd.to_datetime(\n",
    "        df_estacao_temp['data'].astype(str) + ' ' + \n",
    "        df_estacao_temp['hora'].astype(str).str.zfill(2) + ':00:00'\n",
    "    )\n",
    "    \n",
    "    metadata = {\n",
    "        'estacao': estacao,\n",
    "        'target_cols': target_cols,\n",
    "        'feature_cols': feature_cols,\n",
    "        'temporal_features': temporal_features,\n",
    "        'estacao_cols': estacao_columns,\n",
    "        'total_records': len(df_estacao),\n",
    "        'date_range': (\n",
    "            str(df_estacao_temp['timestamp_temp'].min()), \n",
    "            str(df_estacao_temp['timestamp_temp'].max())\n",
    "        ),\n",
    "        'missing_counts': {\n",
    "            col: int(df_estacao[col].isnull().sum()) \n",
    "            for col in target_cols\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(metadata_filename, 'wb') as f:\n",
    "        pickle.dump(metadata, f)\n",
    "    \n",
    "    print(f\"Estacao: {estacao}\")\n",
    "    print(f\"  - {pkl_filename} ({Path(pkl_filename).stat().st_size / 1024 / 1024:.2f} MB)\")\n",
    "    print(f\"  - {metadata_filename}\")\n",
    "    print(f\"  - Registros: {len(df_estacao):,}\")\n",
    "    print(f\"  - Periodo: {df_estacao_temp['timestamp_temp'].min()} ate {df_estacao_temp['timestamp_temp'].max()}\")\n",
    "    print(f\"  - Data e hora: Mantidas em colunas separadas\")\n",
    "    print()\n",
    "\n",
    "print(f\"Total de arquivos criados: {len(estacoes_unicas) * 2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ce37cb",
   "metadata": {},
   "source": [
    "## Resumo do Pipeline de Tratamento\n",
    "\n",
    "Exibe um resumo consolidado de todas as transformacoes realizadas no pipeline, organizado por estacao meteorologica. Para cada estacao, apresenta estatisticas do dataset (periodo, total de registros) e a quantidade de valores faltantes em cada variavel alvo. Este resumo serve como documentacao e validacao do processo de tratamento antes de prosseguir para o notebook de imputacao. Os dados foram exportados em arquivos PKL separados por estacao, cada um mantendo as colunas `data` e `hora` separadas conforme estrutura original do banco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac9eac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"RESUMO DO PIPELINE DE TRATAMENTO - POR ESTACAO\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for estacao in estacoes_unicas:\n",
    "    estacao_col = f'estacao_{estacao}'\n",
    "    if estacao_col not in df_final.columns:\n",
    "        continue\n",
    "    \n",
    "    df_estacao = df_final[df_final[estacao_col] == 1].copy()\n",
    "    \n",
    "    df_temp = df_estacao.copy()\n",
    "    df_temp['timestamp_temp'] = pd.to_datetime(\n",
    "        df_temp['data'].astype(str) + ' ' + \n",
    "        df_temp['hora'].astype(str).str.zfill(2) + ':00:00'\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nEstacao: {estacao}\")\n",
    "    print(f\"  Shape: {df_estacao.shape}\")\n",
    "    print(f\"  Periodo: {df_temp['timestamp_temp'].min()} ate {df_temp['timestamp_temp'].max()}\")\n",
    "    print(f\"  Data e hora: Mantidas em colunas separadas\")\n",
    "    print(f\"\\n  Valores faltantes:\")\n",
    "    for col in target_cols:\n",
    "        missing = df_estacao[col].isnull().sum()\n",
    "        pct = (missing / len(df_estacao) * 100) if len(df_estacao) > 0 else 0\n",
    "        print(f\"    - {col}: {missing:,} ({pct:.2f}%)\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 70)\n",
    "print(f\"Features para imputacao (todas as estacoes):\")\n",
    "print(f\"  - Temporais: {len(temporal_features)}\")\n",
    "print(f\"  - Estacoes (One-Hot): {len(estacao_columns)}\")\n",
    "print(f\"  - Total: {len(feature_cols)}\")\n",
    "print(f\"\\nArquivos gerados: {len(estacoes_unicas)} PKLs de dados + {len(estacoes_unicas)} PKLs de metadata\")\n",
    "print(f\"Formato: Cada PKL mantem colunas 'data' e 'hora' separadas\")\n",
    "print(f\"Proximo passo: executar notebook 02_imputacao_dados.ipynb para cada estacao\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
