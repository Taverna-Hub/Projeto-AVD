{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6e58a43",
   "metadata": {},
   "source": [
    "## Importacao de Bibliotecas\n",
    "\n",
    "Carrega todas as bibliotecas necessarias para o pipeline de tratamento. O modulo `neon_utils` fornece a classe `NeonConnection` que encapsula a logica de conexao com o banco Neon PostgreSQL. As bibliotecas `pandas` e `numpy` sao utilizadas para manipulacao e transformacao dos dados. O modulo `pickle` permite a serializacao dos metadados do tratamento para uso posterior no notebook de imputacao."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48e7dfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sys.path.insert(0, str(Path('..') / 'fastapi'))\n",
    "from services.neon_utils import NeonConnection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2858f4",
   "metadata": {},
   "source": [
    "## Conexao com o Banco de Dados Neon\n",
    "\n",
    "Estabelece a conexao com o banco PostgreSQL hospedado no Neon. A classe `NeonConnection` carrega automaticamente as credenciais do arquivo `.env` localizado na raiz do projeto. O metodo `test_connection()` valida se a conexao foi estabelecida corretamente antes de prosseguir com as operacoes de leitura. A propriedade `engine` retorna um objeto SQLAlchemy Engine que sera utilizado para executar queries SQL via pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fed5d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conexao estabelecida com sucesso\n",
      "Database: neondb\n",
      "Host: ep-still-rain-ahoka4v9.c-3.us-east-1.aws.neon.tech\n"
     ]
    }
   ],
   "source": [
    "env_path = Path('..') / '.env'\n",
    "conn = NeonConnection(str(env_path))\n",
    "\n",
    "if not conn.test_connection():\n",
    "    raise ConnectionError(\"Falha na conexao com o banco Neon\")\n",
    "\n",
    "print(f\"Conexao estabelecida com sucesso\")\n",
    "print(f\"Database: {conn.config['database']}\")\n",
    "print(f\"Host: {conn.config['host']}\")\n",
    "\n",
    "engine = conn.engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab57b9ca",
   "metadata": {},
   "source": [
    "## Carregamento dos Dados da Tabela\n",
    "\n",
    "Executa uma query SQL para carregar todos os registros da tabela `dados_meteorologicos`. A query seleciona apenas as colunas relevantes para o processo de tratamento: identificador unico (`id`), nome da estacao meteorologica (`estacao`), data e hora da medicao, e as tres variaveis meteorologicas de interesse (`temperatura`, `umidade`, `velocidade_vento`). Os dados sao ordenados por estacao e data/hora para garantir a sequencia temporal correta de cada serie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59a4f718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de registros carregados: 526,176\n",
      "Shape do DataFrame: (526176, 7)\n",
      "Quantidade de estacoes: 12\n",
      "\n",
      "Estacoes disponiveis:\n",
      "estacao\n",
      "ARCO VERDE       43848\n",
      "CABROBO          43848\n",
      "CARUARU          43848\n",
      "FLORESTA         43848\n",
      "GARANHUNS        43848\n",
      "IBIMIRIM         43848\n",
      "OURICURI         43848\n",
      "PALMARES         43848\n",
      "PETROLINA        43848\n",
      "SALGUEIRO        43848\n",
      "SERRA TALHADA    43848\n",
      "SURUBIM          43848\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "    id,\n",
    "    estacao,\n",
    "    data,\n",
    "    hora,\n",
    "    temperatura,\n",
    "    umidade,\n",
    "    velocidade_vento\n",
    "FROM dados_meteorologicos\n",
    "ORDER BY estacao, data, hora\n",
    "\"\"\"\n",
    "\n",
    "df_raw = pd.read_sql(query, engine)\n",
    "\n",
    "print(f\"Total de registros carregados: {len(df_raw):,}\")\n",
    "print(f\"Shape do DataFrame: {df_raw.shape}\")\n",
    "print(f\"Quantidade de estacoes: {df_raw['estacao'].nunique()}\")\n",
    "print(f\"\\nEstacoes disponiveis:\")\n",
    "print(df_raw['estacao'].value_counts().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b147b9",
   "metadata": {},
   "source": [
    "## Tratamento de Timestamps\n",
    "\n",
    "Converte as colunas separadas de data e hora em um unico campo timestamp. A coluna `hora` no banco esta armazenada no formato string \"HHMM UTC\" (por exemplo, \"0000 UTC\" para meia-noite). O processamento extrai os dois primeiros caracteres para obter a hora como inteiro (0-23), e em seguida combina com a coluna `data` para criar um timestamp completo no formato datetime do pandas. Este timestamp unificado e essencial para a criacao das features temporais e para manter a ordenacao cronologica dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52506fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Periodo dos dados:\n",
      "  Inicio: 2020-01-01 00:00:00\n",
      "  Fim: 2024-12-31 23:00:00\n",
      "  Duracao: 1826 dias\n"
     ]
    }
   ],
   "source": [
    "df_raw['hora_int'] = df_raw['hora'].str[:2].astype(int)\n",
    "\n",
    "df_raw['timestamp'] = pd.to_datetime(\n",
    "    df_raw['data'].astype(str) + ' ' + \n",
    "    df_raw['hora_int'].astype(str).str.zfill(2) + ':00:00'\n",
    ")\n",
    "\n",
    "print(f\"Periodo dos dados:\")\n",
    "print(f\"  Inicio: {df_raw['timestamp'].min()}\")\n",
    "print(f\"  Fim: {df_raw['timestamp'].max()}\")\n",
    "print(f\"  Duracao: {(df_raw['timestamp'].max() - df_raw['timestamp'].min()).days} dias\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb766296",
   "metadata": {},
   "source": [
    "## Analise de Valores Ausentes\n",
    "\n",
    "Quantifica a presenca de valores nulos nas tres variaveis meteorologicas de interesse. Esta analise e fundamental para entender a magnitude do problema de dados faltantes que sera tratado no notebook de imputacao. O percentual de valores ausentes em cada coluna determina a viabilidade e a estrategia de imputacao a ser adotada. Valores faltantes podem ocorrer devido a falhas nos sensores, problemas de transmissao ou periodos de manutencao das estacoes meteorologicas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "801a2559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analise de Valores Ausentes:\n",
      "          Coluna  Valores Faltantes  Percentual (%)\n",
      "     temperatura             147124           27.96\n",
      "         umidade             183691           34.91\n",
      "velocidade_vento             148572           28.24\n"
     ]
    }
   ],
   "source": [
    "target_columns = ['temperatura', 'umidade', 'velocidade_vento']\n",
    "\n",
    "missing_counts = df_raw[target_columns].isnull().sum()\n",
    "missing_percentages = (missing_counts / len(df_raw) * 100).round(2)\n",
    "\n",
    "missing_analysis = pd.DataFrame({\n",
    "    'Coluna': missing_counts.index,\n",
    "    'Valores Faltantes': missing_counts.values,\n",
    "    'Percentual (%)': missing_percentages.values\n",
    "})\n",
    "\n",
    "print(\"Analise de Valores Ausentes:\")\n",
    "print(missing_analysis.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb512cb",
   "metadata": {},
   "source": [
    "## Engenharia de Features Temporais\n",
    "\n",
    "Cria variaveis derivadas do timestamp que capturam padroes temporais relevantes para o processo de imputacao. As features incluem:\n",
    "\n",
    "- **ano, mes, dia, hora**: Componentes basicos do timestamp para capturar sazonalidades\n",
    "- **dia_ano**: Dia do ano (1-365/366), util para capturar ciclos anuais\n",
    "- **dia_semana**: Dia da semana (0-6), onde 0 representa segunda-feira\n",
    "- **hora_sin, hora_cos**: Codificacao ciclica da hora usando funcoes seno e cosseno, garantindo que a hora 23 seja proxima da hora 0\n",
    "- **mes_sin, mes_cos**: Codificacao ciclica do mes para capturar sazonalidade anual de forma continua\n",
    "\n",
    "A codificacao ciclica e preferivel a codificacao linear para variaveis periodicas, pois preserva a relacao de proximidade entre valores nas extremidades do ciclo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31d31fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features temporais criadas: 10\n",
      "Shape apos feature engineering: (526176, 18)\n"
     ]
    }
   ],
   "source": [
    "df = df_raw.sort_values(['estacao', 'timestamp']).copy()\n",
    "\n",
    "df['ano'] = df['timestamp'].dt.year\n",
    "df['mes'] = df['timestamp'].dt.month\n",
    "df['dia'] = df['timestamp'].dt.day\n",
    "df['hora'] = df['hora_int']\n",
    "df['dia_ano'] = df['timestamp'].dt.dayofyear\n",
    "df['dia_semana'] = df['timestamp'].dt.dayofweek\n",
    "\n",
    "df['hora_sin'] = np.sin(2 * np.pi * df['hora'] / 24)\n",
    "df['hora_cos'] = np.cos(2 * np.pi * df['hora'] / 24)\n",
    "df['mes_sin'] = np.sin(2 * np.pi * df['mes'] / 12)\n",
    "df['mes_cos'] = np.cos(2 * np.pi * df['mes'] / 12)\n",
    "\n",
    "print(f\"Features temporais criadas: 10\")\n",
    "print(f\"Shape apos feature engineering: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1e6b6b",
   "metadata": {},
   "source": [
    "## Codificacao One-Hot da Variavel Categorica\n",
    "\n",
    "Aplica codificacao One-Hot Encoding na coluna `estacao`, transformando a variavel categorica em um conjunto de variaveis binarias. Cada estacao meteorologica passa a ser representada por uma coluna propria, contendo valor 1 quando o registro pertence aquela estacao e 0 caso contrario. Esta transformacao e necessaria porque algoritmos de machine learning, incluindo o IterativeImputer, requerem entradas numericas. A codificacao One-Hot preserva a natureza nominal da variavel (nao ha ordem entre as estacoes) e permite que o modelo de imputacao aprenda padroes especificos de cada localidade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc94d295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colunas One-Hot criadas: 12\n",
      "Shape apos encoding: (526176, 29)\n",
      "\n",
      "Colunas de estacao:\n",
      "  - estacao_ARCO VERDE\n",
      "  - estacao_CABROBO\n",
      "  - estacao_CARUARU\n",
      "  - estacao_FLORESTA\n",
      "  - estacao_GARANHUNS\n",
      "  - estacao_IBIMIRIM\n",
      "  - estacao_OURICURI\n",
      "  - estacao_PALMARES\n",
      "  - estacao_PETROLINA\n",
      "  - estacao_SALGUEIRO\n",
      "  - estacao_SERRA TALHADA\n",
      "  - estacao_SURUBIM\n"
     ]
    }
   ],
   "source": [
    "df_encoded = pd.get_dummies(df, columns=['estacao'], prefix='estacao', dtype=int)\n",
    "\n",
    "estacao_columns = [col for col in df_encoded.columns if col.startswith('estacao_')]\n",
    "\n",
    "print(f\"Colunas One-Hot criadas: {len(estacao_columns)}\")\n",
    "print(f\"Shape apos encoding: {df_encoded.shape}\")\n",
    "print(f\"\\nColunas de estacao:\")\n",
    "for col in estacao_columns:\n",
    "    print(f\"  - {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58983322",
   "metadata": {},
   "source": [
    "## Selecao e Organizacao das Colunas Finais\n",
    "\n",
    "Organiza o DataFrame final selecionando apenas as colunas necessarias para o processo de imputacao. A estrutura final inclui:\n",
    "\n",
    "- **Identificadores**: `id` (chave primaria do banco) e `timestamp` (referencia temporal)\n",
    "- **Colunas alvo**: `temperatura`, `umidade`, `velocidade_vento` - variaveis que contem valores faltantes a serem imputados\n",
    "- **Features**: variaveis temporais (10 colunas) e variaveis de estacao (12 colunas One-Hot)\n",
    "\n",
    "Esta organizacao separa claramente o que sera imputado (colunas alvo) do que sera usado como informacao auxiliar (features), facilitando o processo no notebook de imputacao."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b5a85e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset final preparado\n",
      "  Total de registros: 526,176\n",
      "  Colunas alvo: 3\n",
      "  Features temporais: 10\n",
      "  Features de estacao: 12\n",
      "  Total de colunas: 27\n"
     ]
    }
   ],
   "source": [
    "target_cols = ['temperatura', 'umidade', 'velocidade_vento']\n",
    "\n",
    "temporal_features = [\n",
    "    'ano', 'mes', 'dia', 'hora', 'dia_ano', 'dia_semana',\n",
    "    'hora_sin', 'hora_cos', 'mes_sin', 'mes_cos'\n",
    "]\n",
    "\n",
    "feature_cols = temporal_features + estacao_columns\n",
    "\n",
    "final_columns = ['id', 'timestamp'] + target_cols + feature_cols\n",
    "df_final = df_encoded[final_columns].copy()\n",
    "\n",
    "print(f\"Dataset final preparado\")\n",
    "print(f\"  Total de registros: {len(df_final):,}\")\n",
    "print(f\"  Colunas alvo: {len(target_cols)}\")\n",
    "print(f\"  Features temporais: {len(temporal_features)}\")\n",
    "print(f\"  Features de estacao: {len(estacao_columns)}\")\n",
    "print(f\"  Total de colunas: {len(final_columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4698122",
   "metadata": {},
   "source": [
    "## Exportacao do Dataset Tratado\n",
    "\n",
    "Serializa o DataFrame tratado e os metadados do pipeline para arquivos pickle. O arquivo `dados_tratados.pkl` contem o dataset completo pronto para imputacao. O arquivo `metadata_tratamento.pkl` armazena informacoes essenciais sobre o tratamento realizado, incluindo:\n",
    "\n",
    "- Nomes das colunas alvo e features\n",
    "- Lista de colunas One-Hot das estacoes\n",
    "- Estatisticas do dataset (total de registros, periodo, contagem de valores faltantes)\n",
    "\n",
    "Estes metadados sao carregados pelo notebook de imputacao para garantir consistencia no processamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7b540df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivos exportados:\n",
      "  - dados_tratados.pkl (98.35 MB)\n",
      "  - metadata_tratamento.pkl\n"
     ]
    }
   ],
   "source": [
    "df_final.to_pickle('dados_tratados.pkl')\n",
    "\n",
    "metadata = {\n",
    "    'target_cols': target_cols,\n",
    "    'feature_cols': feature_cols,\n",
    "    'temporal_features': temporal_features,\n",
    "    'estacao_cols': estacao_columns,\n",
    "    'total_records': len(df_final),\n",
    "    'date_range': (\n",
    "        str(df_final['timestamp'].min()), \n",
    "        str(df_final['timestamp'].max())\n",
    "    ),\n",
    "    'missing_counts': {\n",
    "        col: int(df_final[col].isnull().sum()) \n",
    "        for col in target_cols\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('metadata_tratamento.pkl', 'wb') as f:\n",
    "    pickle.dump(metadata, f)\n",
    "\n",
    "print(\"Arquivos exportados:\")\n",
    "print(f\"  - dados_tratados.pkl ({Path('dados_tratados.pkl').stat().st_size / 1024 / 1024:.2f} MB)\")\n",
    "print(f\"  - metadata_tratamento.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ce37cb",
   "metadata": {},
   "source": [
    "## Resumo do Pipeline de Tratamento\n",
    "\n",
    "Exibe um resumo consolidado de todas as transformacoes realizadas no pipeline, incluindo estatisticas do dataset final e a quantidade de valores faltantes em cada variavel alvo. Este resumo serve como documentacao e validacao do processo de tratamento antes de prosseguir para o notebook de imputacao."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aac9eac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "RESUMO DO PIPELINE DE TRATAMENTO\n",
      "======================================================================\n",
      "\n",
      "Dataset:\n",
      "  Shape: (526176, 27)\n",
      "  Periodo: 2020-01-01 00:00:00 ate 2024-12-31 23:00:00\n",
      "\n",
      "Variaveis alvo (com valores faltantes):\n",
      "  - temperatura: 147,124 (27.96%)\n",
      "  - umidade: 183,691 (34.91%)\n",
      "  - velocidade_vento: 148,572 (28.24%)\n",
      "\n",
      "Features para imputacao:\n",
      "  - Temporais: 10\n",
      "  - Estacoes (One-Hot): 12\n",
      "  - Total: 22\n",
      "\n",
      "Proximo passo: executar notebook 02_imputacao_dados.ipynb\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"RESUMO DO PIPELINE DE TRATAMENTO\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nDataset:\")\n",
    "print(f\"  Shape: {df_final.shape}\")\n",
    "print(f\"  Periodo: {df_final['timestamp'].min()} ate {df_final['timestamp'].max()}\")\n",
    "print(f\"\\nVariaveis alvo (com valores faltantes):\")\n",
    "for col in target_cols:\n",
    "    missing = df_final[col].isnull().sum()\n",
    "    pct = (missing / len(df_final) * 100)\n",
    "    print(f\"  - {col}: {missing:,} ({pct:.2f}%)\")\n",
    "print(f\"\\nFeatures para imputacao:\")\n",
    "print(f\"  - Temporais: {len(temporal_features)}\")\n",
    "print(f\"  - Estacoes (One-Hot): {len(estacao_columns)}\")\n",
    "print(f\"  - Total: {len(feature_cols)}\")\n",
    "print(f\"\\nProximo passo: executar notebook 02_imputacao_dados.ipynb\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
